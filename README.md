# Azure Machine Learning - LoRA Fine-tuning for Small Language Models

Fine-tune open-source small language models using LoRA on Azure Machine Learning Studio with the Databricks Dolly 15K dataset.

## Overview

- **Model**: Phi-4-mini-instruct (3.8B params, 128K context)
- **Dataset**: Databricks Dolly 15K (15K instruction-response pairs)
- **Method**: LoRA with 4-bit quantization

## ğŸ¯ Features

- âœ… Parameter-efficient fine-tuning with LoRA
- âœ… 4-bit quantization for memory efficiency (~5GB GPU memory)
- âœ… Databricks Dolly 15K dataset (8 instruction categories)
- âœ… Model comparison script (base vs fine-tuned)
- âœ… Azure ML integration with MLflow tracking
- âœ… Training time: ~1-2 hours on V100

## ğŸ—ï¸ Architecture & Flow

```mermaid
graph TB
    subgraph Local["ğŸ’» Local Development"]
        A["ğŸ“ Configuration Files<br/>training_config.yaml<br/>compute_config.yaml"]
        B["ğŸ Training Scripts<br/>train.py / train_cpu.py"]
        C["ğŸ“Š Job Submission<br/>submit_training_job.py"]
    end
    
    subgraph Azure["â˜ï¸ Azure Machine Learning"]
        D["ğŸ–¥ï¸ Compute Management<br/>â€¢ Create/Start VM<br/>â€¢ GPU: Standard_NC6s_v3<br/>â€¢ CPU: Standard_E4ds_v4"]
        E["ğŸ”§ Environment Setup<br/>â€¢ Install dependencies<br/>â€¢ Load model & dataset<br/>â€¢ Configure LoRA"]
        F["ğŸš€ Training Execution<br/>â€¢ Fine-tune with LoRA<br/>â€¢ Track with MLflow<br/>â€¢ Save checkpoints"]
        G["ğŸ’¾ Model Registry<br/>â€¢ Store fine-tuned model<br/>â€¢ Version management<br/>â€¢ Deployment ready"]
        H["ğŸ“ˆ Monitoring<br/>â€¢ Training metrics<br/>â€¢ Resource usage<br/>â€¢ Cost tracking"]
    end
    
    subgraph Data["ğŸ“š External Resources"]
        I["ğŸ¤— HuggingFace<br/>â€¢ Phi-4-mini-instruct<br/>â€¢ Dolly 15K dataset"]
    end
    
    A --> C
    B --> C
    C -->|"1. Submit Job"| D
    D -->|"2. Provision VM"| E
    E -->|"3. Download"| I
    E -->|"4. Start Training"| F
    F -->|"5. Log Metrics"| H
    F -->|"6. Save Model"| G
    G -->|"7. Shutdown VM"| D
    
    style Local fill:#e1f5ff
    style Azure fill:#fff4e1
    style Data fill:#f0f0f0
```

## Project Structure

```
root/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py                 # Fine-tuning script
â”‚   â”œâ”€â”€ evaluate.py              # Evaluation script
â”‚   â”œâ”€â”€ compare_models.py        # Compare base vs fine-tuned
â”‚   â””â”€â”€ utils.py                 # Utility functions
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ submit_training_job.py   # Azure ML training job
â”‚   â””â”€â”€ submit_eval_job.py       # Azure ML evaluation job
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ training_config.yaml     # Training configuration
â”‚   â””â”€â”€ compute_config.yaml      # Azure ML compute config
â””â”€â”€ environment/
    â”œâ”€â”€ conda.yaml               # Conda environment
    â””â”€â”€ requirements.txt         # Python dependencies
```

## Dataset: Databricks Dolly 15K

15,011 human-generated instruction-response pairs across 8 categories (Open QA, Closed QA, Brainstorming, Classification, Summarization, Information Extraction, Creative Writing, General QA).

- License: CC BY-SA 3.0
- Local samples: `data/train.jsonl` (20), `data/validation.jsonl` (5)
- Training auto-downloads full dataset from HuggingFace

## ğŸ“‹ Prerequisites

- Azure ML workspace
- Compute: GPU (Standard_NC6s_v3) or CPU (Standard_E4ds_v4)
- Python 3.9+

## ğŸ“¦ Installation

### Option 1: Fast Installation with uv âš¡ (Recommended)

[uv](https://github.com/astral-sh/uv) is 10-100x faster than pip, written in Rust by Astral.

**Windows (PowerShell):**
```powershell
.\scripts\generate_requirements_uv.ps1
```

**Linux/Mac (Bash):**
```bash
chmod +x scripts/generate_requirements_uv.sh
./scripts/generate_requirements_uv.sh
```

### Option 2: Standard pip Installation

**Windows (PowerShell):**
```powershell
.\scripts\generate_requirements.ps1
```

**Linux/Mac (Bash):**
```bash
chmod +x scripts/generate_requirements.sh
./scripts/generate_requirements.sh
```

### Option 3: Use Existing Requirements

```bash
pip install -r environment/requirements.txt
```

## ğŸš€ Quick Start

```bash
# 1. Install dependencies
.\scripts\generate_requirements_uv.ps1  # or generate_requirements.ps1

# 2. Configure Azure (edit .env with your details)
cp .env.template .env

# 3. Submit training
cd jobs
python submit_training_job.py        # GPU: 15K samples, ~$1.80
# python submit_training_job_cpu.py  # CPU: 1K samples, ~$1.16

# 4. Compare models
python src/compare_models.py --adapter_path ./outputs/final_model
```

## Usage

```bash
# Train
python jobs/submit_training_job.py  # or submit_training_job_cpu.py

# Compare models
python src/compare_models.py --adapter_path ./outputs/final_model

# Evaluate
python jobs/submit_eval_job.py --model_path ./outputs/final_model

# Local training (optional)
python src/train.py --max_samples 100
```

## âš™ï¸ Configuration

Edit `config/training_config.yaml` (GPU) or `config/training_config_cpu.yaml` (CPU):

```yaml
model:
  name: "microsoft/Phi-4-mini-instruct"
data:
  dataset_name: "databricks/databricks-dolly-15k"
  max_samples: null  # GPU: null (15K), CPU: 1000
lora:
  r: 16              # GPU: 16, CPU: 8
training:
  batch_size: 4      # GPU: 4, CPU: 1
  max_seq_length: 512  # GPU: 512, CPU: 256
```

## Model Details

**Phi-4-mini-instruct**: 3.8B params, 128K context, MIT license

| Config | GPU | CPU |
|--------|-----|-----|
| LoRA rank | 16 | 8 |
| Trainable params | 8.4M | 4.2M |
| Memory | 5GB | 16GB |
| Time | 1-2h | 2-4h |
| Dataset | 15K | 1K |
| Quantization | 4-bit | None |

## âš ï¸ Important Notes

### Azure ML Requirements
- Requires active Azure subscription with ML workspace
- Create compute cluster before submitting jobs
- Configure auto-shutdown to avoid idle charges

### Cost Estimates
- **GPU**: ~$0.90/hour (1-2 hours) = ~$1.80 total
- **CPU**: ~$0.29/hour (2-4 hours) = ~$0.58-$1.16 total

### Training Tips
- **Test first**: Start with `--max_samples 100` before full training
- **GPU**: Full 15K dataset, ~5GB GPU memory
- **CPU**: Limit to 1K samples, ~16GB RAM, no quantization

## ğŸ”§ Troubleshooting

**Out of Memory:**
- GPU: Reduce `batch_size` to 2 or 1
- CPU: Reduce `max_samples` to 500 or `max_seq_length` to 128

**Job Submission Fails:**
- Run from project root, not subdirectories
- Verify compute cluster exists in Azure ML Studio
- Run `az login` to authenticate

**Slow Training:**
- CPU is 10-20x slower than GPU - use `max_samples=100` for testing

## Resources

- [Phi-4 Model Card](https://huggingface.co/microsoft/Phi-4-mini-instruct)
- [Dolly Dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
- [Azure ML Docs](https://learn.microsoft.com/azure/machine-learning/)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)

## License

MIT
