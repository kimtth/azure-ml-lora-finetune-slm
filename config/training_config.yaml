# Model Configuration
model:
  name: "microsoft/Phi-4-mini-instruct"
  description: "Microsoft Phi-4 mini - 3.8B parameter instruction-tuned model with 128K context"

# Data Configuration
data:
  dataset_name: "databricks/databricks-dolly-15k"
  train_split: "train"
  validation_size: 0.05
  max_samples: null  # Set to limit dataset size for testing

# LoRA Configuration
lora:
  r: 16                          # LoRA rank (smaller = fewer parameters)
  lora_alpha: 32                 # LoRA alpha (typically 2*r)
  lora_dropout: 0.05             # Dropout probability for LoRA layers
  target_modules:                # Modules to apply LoRA to
    - q_proj
    - k_proj
    - v_proj
    - o_proj
  bias: "none"                   # Bias handling: none, all, lora_only
  task_type: "CAUSAL_LM"         # Task type for LoRA

# Quantization Configuration
quantization:
  load_in_4bit: true             # Enable 4-bit quantization
  bnb_4bit_use_double_quant: true  # Use nested quantization
  bnb_4bit_quant_type: "nf4"     # Quantization type: nf4 or fp4
  bnb_4bit_compute_dtype: "bfloat16"  # Compute dtype: bfloat16 or float16

# Training Configuration
training:
  # Data
  max_seq_length: 512            # Maximum sequence length
  
  # Optimization
  num_epochs: 3                  # Number of training epochs
  batch_size: 4                  # Training batch size per device
  eval_batch_size: 4             # Evaluation batch size per device
  gradient_accumulation_steps: 4 # Gradient accumulation steps
  learning_rate: 2.0e-4          # Peak learning rate
  weight_decay: 0.01             # Weight decay coefficient
  warmup_steps: 100              # Number of warmup steps
  
  # Optimizer
  optimizer: "paged_adamw_8bit"  # Optimizer: adamw, paged_adamw_8bit
  lr_scheduler_type: "cosine"    # Learning rate scheduler
  
  # Memory Optimization
  gradient_checkpointing: true   # Enable gradient checkpointing
  fp16: false                    # Use FP16 training
  bf16: true                     # Use BF16 training (recommended for A100)
  
  # Logging and Checkpointing
  logging_steps: 10              # Log every N steps
  save_steps: 100                # Save checkpoint every N steps
  eval_steps: 100                # Evaluate every N steps
  save_total_limit: 3            # Maximum number of checkpoints to keep
  
  # Evaluation
  eval_strategy: "steps"         # When to evaluate: no, steps, epoch
  load_best_model_at_end: true   # Load best model at end of training
  metric_for_best_model: "loss"  # Metric to use for best model
  greater_is_better: false       # Whether metric should be maximized
  
  # Miscellaneous
  seed: 42                       # Random seed for reproducibility
  dataloader_num_workers: 4      # Number of data loading workers

# Evaluation Configuration
evaluation:
  max_new_tokens: 256            # Maximum tokens to generate
  temperature: 0.7               # Sampling temperature
  top_p: 0.9                     # Top-p (nucleus) sampling parameter
  top_k: 50                      # Top-k sampling parameter
  do_sample: true                # Enable sampling (vs greedy decoding)
  repetition_penalty: 1.1        # Repetition penalty
