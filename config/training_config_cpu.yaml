# CPU-Optimized Training Configuration
# For use on Standard_E4ds_v4 (4 cores, 32GB RAM)

# Model Configuration
model:
  name: "microsoft/Phi-4-mini-instruct"
  description: "Microsoft Phi-4 mini - CPU training optimized"

# Data Configuration
data:
  dataset_name: "databricks/databricks-dolly-15k"
  train_split: "train"
  validation_size: 0.05
  max_samples: 1000  # Start with subset for CPU training

# LoRA Configuration (smaller for CPU)
lora:
  r: 8                           # Reduced rank for faster training
  lora_alpha: 16                 # Proportionally reduced
  lora_dropout: 0.05
  target_modules:
    - q_proj
    - v_proj
  bias: "none"
  task_type: "CAUSAL_LM"

# NO Quantization for CPU
quantization:
  load_in_4bit: false            # 4-bit not supported on CPU
  bnb_4bit_use_double_quant: false
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float32"

# CPU-Optimized Training Configuration
training:
  # Data
  max_seq_length: 256            # Reduced for CPU
  
  # Optimization
  num_epochs: 2                  # Fewer epochs for CPU
  batch_size: 1                  # Small batch for CPU
  eval_batch_size: 1
  gradient_accumulation_steps: 16  # Simulate larger batch
  learning_rate: 3.0e-4          # Slightly higher for stability
  weight_decay: 0.01
  warmup_steps: 50
  
  # Optimizer
  optimizer: "adamw_torch"       # Standard AdamW for CPU
  lr_scheduler_type: "cosine"
  
  # Memory Optimization
  gradient_checkpointing: true
  fp16: false                    # No mixed precision on CPU
  bf16: false
  
  # Logging and Checkpointing
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  save_total_limit: 2
  
  # Evaluation
  eval_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "loss"
  greater_is_better: false
  
  # Miscellaneous
  seed: 42
  dataloader_num_workers: 2      # Use 2 cores for data loading

# Evaluation Configuration
evaluation:
  max_new_tokens: 128            # Reduced for CPU
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true
  repetition_penalty: 1.1
